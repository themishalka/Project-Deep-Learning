{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0570eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional, Sequence, Tuple, Union, Dict\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model as keras_load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import keras_tuner as kt\n",
    "from dataclasses import dataclass, asdict\n",
    "import random\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492f5fe",
   "metadata": {},
   "source": [
    "Step 1: We load the expression, gene and sample data we saved after the pre-processing we applied in R. We aim to predict tumor grade from the RNAseq data, so we examine potential class imbalance in the tumor grade sample metadata column, and we remove NA values in tumors with unidentified grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e7e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load expression, genes and samples data\n",
    "expression=pd.read_csv(\"C:/Users/joann/Desktop/M2/Deep_Learning/merged_expression.csv\",index_col=0)\n",
    "genes=pd.read_csv(\"C:/Users/joann/Desktop/M2/Deep_Learning/merged_genes.csv\")\n",
    "samples = pyreadr.read_r(\"C:/Users/joann/Desktop/M2/Deep_Learning/merged_samples.rds\")\n",
    "samples = samples[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b30a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ENSG00000276168.1', 'ENSG00000129824.16', 'ENSG00000133048.13',\n",
      "       'ENSG00000012223.13', 'ENSG00000198695.2'],\n",
      "      dtype='object')\n",
      "                              ENSG00000276168.1  ENSG00000129824.16  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07           8.423027           13.253145   \n",
      "TCGA-DU-7015-01A-11R-2027-07           8.252623            5.532866   \n",
      "\n",
      "                              ENSG00000133048.13  ENSG00000012223.13  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07            7.491808            4.879747   \n",
      "TCGA-DU-7015-01A-11R-2027-07            8.707316            5.087127   \n",
      "\n",
      "                              ENSG00000198695.2  ENSG00000228253.1  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07          15.950897          12.763416   \n",
      "TCGA-DU-7015-01A-11R-2027-07          16.922914          13.431528   \n",
      "\n",
      "                              ENSG00000198763.3  ENSG00000133110.15  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07          17.025877            5.074539   \n",
      "TCGA-DU-7015-01A-11R-2027-07          17.670450            4.085189   \n",
      "\n",
      "                              ENSG00000198899.2  ENSG00000248527.1  ...  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07          16.826818          13.863041  ...   \n",
      "TCGA-DU-7015-01A-11R-2027-07          17.706981          14.950704  ...   \n",
      "\n",
      "                              ENSG00000179796.12  ENSG00000240087.3  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07            9.234303           4.836707   \n",
      "TCGA-DU-7015-01A-11R-2027-07            9.382020           5.434878   \n",
      "\n",
      "                              ENSG00000157087.20  ENSG00000164128.7  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07           11.285052           5.473512   \n",
      "TCGA-DU-7015-01A-11R-2027-07           12.349215           5.809448   \n",
      "\n",
      "                              ENSG00000170075.9  ENSG00000188783.6  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07          14.468218          11.255899   \n",
      "TCGA-DU-7015-01A-11R-2027-07          13.080774          10.141038   \n",
      "\n",
      "                              ENSG00000112333.12  ENSG00000166819.12  \\\n",
      "TCGA-HT-7468-01A-11R-2027-07            8.641767            6.874675   \n",
      "TCGA-DU-7015-01A-11R-2027-07            8.065001            7.409357   \n",
      "\n",
      "                              ENSG00000116194.13  ENSG00000130988.13  \n",
      "TCGA-HT-7468-01A-11R-2027-07            8.731906            7.238853  \n",
      "TCGA-DU-7015-01A-11R-2027-07            9.466173            7.052062  \n",
      "\n",
      "[2 rows x 2000 columns]\n",
      "                                                   barcode       patient  \\\n",
      "rownames                                                                   \n",
      "TCGA-HT-7468-01A-11R-2027-07  TCGA-HT-7468-01A-11R-2027-07  TCGA-HT-7468   \n",
      "TCGA-DU-7015-01A-11R-2027-07  TCGA-DU-7015-01A-11R-2027-07  TCGA-DU-7015   \n",
      "\n",
      "                                        sample shortLetterCode  \\\n",
      "rownames                                                         \n",
      "TCGA-HT-7468-01A-11R-2027-07  TCGA-HT-7468-01A              TP   \n",
      "TCGA-DU-7015-01A-11R-2027-07  TCGA-DU-7015-01A              TP   \n",
      "\n",
      "                                       definition sample_submitter_id  \\\n",
      "rownames                                                                \n",
      "TCGA-HT-7468-01A-11R-2027-07  Primary solid Tumor    TCGA-HT-7468-01A   \n",
      "TCGA-DU-7015-01A-11R-2027-07  Primary solid Tumor    TCGA-DU-7015-01A   \n",
      "\n",
      "                              intermediate_dimension tumor_descriptor  \\\n",
      "rownames                                                                \n",
      "TCGA-HT-7468-01A-11R-2027-07                     0.7          Primary   \n",
      "TCGA-DU-7015-01A-11R-2027-07                     1.0          Primary   \n",
      "\n",
      "                                                 sample_id  \\\n",
      "rownames                                                     \n",
      "TCGA-HT-7468-01A-11R-2027-07  TCGA-HT-7468-01A-11R-2027-07   \n",
      "TCGA-DU-7015-01A-11R-2027-07  TCGA-DU-7015-01A-11R-2027-07   \n",
      "\n",
      "                                             pathology_report_uuid  ...  \\\n",
      "rownames                                                            ...   \n",
      "TCGA-HT-7468-01A-11R-2027-07  f56fcc2c-db3d-4ae5-b8dc-975b98fb4675  ...   \n",
      "TCGA-DU-7015-01A-11R-2027-07  15dbad7b-e2ae-42b0-92a1-5ccfc3090c2d  ...   \n",
      "\n",
      "                             paper_IDH.specific.RNA.Expression.Cluster  \\\n",
      "rownames                                                                 \n",
      "TCGA-HT-7468-01A-11R-2027-07                                 IDHmut-R1   \n",
      "TCGA-DU-7015-01A-11R-2027-07                                 IDHmut-R3   \n",
      "\n",
      "                              paper_Pan.Glioma.DNA.Methylation.Cluster  \\\n",
      "rownames                                                                 \n",
      "TCGA-HT-7468-01A-11R-2027-07                                      LGm3   \n",
      "TCGA-DU-7015-01A-11R-2027-07                                      LGm2   \n",
      "\n",
      "                             paper_IDH.specific.DNA.Methylation.Cluster  \\\n",
      "rownames                                                                  \n",
      "TCGA-HT-7468-01A-11R-2027-07                                  IDHmut-K3   \n",
      "TCGA-DU-7015-01A-11R-2027-07                                  IDHmut-K2   \n",
      "\n",
      "                             paper_Supervised.DNA.Methylation.Cluster  \\\n",
      "rownames                                                                \n",
      "TCGA-HT-7468-01A-11R-2027-07                                    Codel   \n",
      "TCGA-DU-7015-01A-11R-2027-07                              G-CIMP-high   \n",
      "\n",
      "                              paper_Random.Forest.Sturm.Cluster  \\\n",
      "rownames                                                          \n",
      "TCGA-HT-7468-01A-11R-2027-07                                IDH   \n",
      "TCGA-DU-7015-01A-11R-2027-07                                IDH   \n",
      "\n",
      "                             paper_RPPA.cluster  \\\n",
      "rownames                                          \n",
      "TCGA-HT-7468-01A-11R-2027-07                 K2   \n",
      "TCGA-DU-7015-01A-11R-2027-07                 K2   \n",
      "\n",
      "                             paper_Telomere.length.estimate.in.blood.normal..Kb.  \\\n",
      "rownames                                                                           \n",
      "TCGA-HT-7468-01A-11R-2027-07                                             5.4533    \n",
      "TCGA-DU-7015-01A-11R-2027-07                                                NaN    \n",
      "\n",
      "                              paper_Telomere.length.estimate.in.tumor..Kb.  \\\n",
      "rownames                                                                     \n",
      "TCGA-HT-7468-01A-11R-2027-07                                        2.5218   \n",
      "TCGA-DU-7015-01A-11R-2027-07                                           NaN   \n",
      "\n",
      "                             tumor_type method_of_diagnosis  \n",
      "rownames                                                     \n",
      "TCGA-HT-7468-01A-11R-2027-07        LGG                 NaN  \n",
      "TCGA-DU-7015-01A-11R-2027-07        LGG                 NaN  \n",
      "\n",
      "[2 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "#check what the expression, genes and samples data look like\n",
    "print(expression.columns[:5])\n",
    "print(expression.head(2))\n",
    "print(samples.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbcc870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(925, 114)\n",
      "tumor_grade\n",
      "G4    391\n",
      "G3    216\n",
      "G2    211\n",
      "Name: count, dtype: int64\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "#explore the samples metadata\n",
    "print(samples.shape)\n",
    "# list of column names\n",
    "samples.columns.tolist()\n",
    "\n",
    "#check for potential class imbalance in tumor grade\n",
    "print(samples[\"tumor_grade\"].value_counts())\n",
    "\n",
    "#count how many NA values are in the tumor grade column\n",
    "print(samples[\"tumor_grade\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7695a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we exclude samples with NA tumor grade\n",
    "samples_all = samples.copy() #keep original samples dataframe\n",
    "expression_all = expression.copy() #keep original expression dataframe\n",
    "\n",
    "samples_sup = samples_all.dropna(subset=[\"tumor_grade\"])\n",
    "expression_sup = expression_all.loc[samples_sup.index]\n",
    "\n",
    "assert expression_sup.shape[0] == samples_sup.shape[0], \"Mismatch in number of samples between expression and samples dataframes after dropping NA tumor grades.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09d5ad",
   "metadata": {},
   "source": [
    "Step 2: We configure the model and ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e796cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    out_dir: str = \"C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # labels\n",
    "    label_col: str = \"tumor_grade\"\n",
    "    label_map: dict = None  # set in main if None\n",
    "\n",
    "    # splits\n",
    "    test_size: float = 0.15\n",
    "    n_folds: int = 9  # 9-fold CV on train+val portion\n",
    "\n",
    "    # training/tuning\n",
    "    max_epochs: int = 100\n",
    "    early_stop_patience: int = 5\n",
    "\n",
    "    # tuner\n",
    "    hyperband_factor: int = 3\n",
    "    objective: str = \"val_accuracy\"\n",
    "\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # makes TF ops more deterministic when possible\n",
    "    try:\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def make_run_dir(base_dir: str) -> str:\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(base_dir, ts)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d10a2",
   "metadata": {},
   "source": [
    "Step 3: We assign the classes. Grade II tumors will be 0, grade III will be 1 and grade IV will be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b72236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y(samples_sup: pd.DataFrame, label_col: str, label_map: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert tumor grade strings to integer classes.\n",
    "    Example mapping: {\"G2\":0, \"G3\":1, \"G4\":2}\n",
    "    \"\"\"\n",
    "    y = samples_sup[label_col].astype(str).map(label_map)\n",
    "    if y.isna().any():\n",
    "        bad = samples_sup.loc[y.isna(), label_col].unique()\n",
    "        raise ValueError(f\"Found unmapped labels: {bad}\")\n",
    "    return y.astype(int).to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3a5fbb",
   "metadata": {},
   "source": [
    "Step 4: We construct our  splits: we will have a hold-out test set, and run a 9-fold cross-validation on the rest of the data. In the cross-validation we include training and validation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6420581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_holdout_test(X: pd.DataFrame, y: np.ndarray, cfg: Config):\n",
    "    \"\"\"\n",
    "    Hold out ONE stratified test set.\n",
    "    Remaining data is used for 9-fold CV (train/val folds).\n",
    "    \"\"\"\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=cfg.test_size,\n",
    "        random_state=cfg.seed,\n",
    "        stratify=y\n",
    "    )\n",
    "    return X_trainval, X_test, y_trainval, y_test\n",
    "\n",
    "def save_test_ids(run_dir: str, X_test: pd.DataFrame) -> None:\n",
    "    pd.Index(X_test.index).to_series().to_csv(os.path.join(run_dir, \"test_ids.csv\"), index=False)\n",
    "\n",
    "def save_fold_ids(run_dir: str, fold: int, X_train: pd.DataFrame, X_val: pd.DataFrame) -> None:\n",
    "    fold_dir = os.path.join(run_dir, f\"fold_{fold:02d}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    pd.Index(X_train.index).to_series().to_csv(os.path.join(fold_dir, \"train_ids.csv\"), index=False)\n",
    "    pd.Index(X_val.index).to_series().to_csv(os.path.join(fold_dir, \"val_ids.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d257ba",
   "metadata": {},
   "source": [
    "Step 5: We standardize the data, fitting on test data only to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c663e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_train_val(X_train: pd.DataFrame, X_val: pd.DataFrame):\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_train_s = scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_val_s = scaler.transform(X_val).astype(np.float32)\n",
    "    return X_train_s, X_val_s, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97070e",
   "metadata": {},
   "source": [
    "Step 6: We build our multi-layer perceptron model (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b49462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp: kt.HyperParameters, input_dim: int, num_classes: int = 3) -> keras.Model:\n",
    "    \"\"\"\n",
    "    MLP with tunable depth and two architecture modes:\n",
    "      1) Free: each layer can have its own units (picked from a grid)\n",
    "      2) Pyramid: pick a base width; each next layer is half the previous (e.g. 32 -> 16 -> 8)\n",
    "    \"\"\"\n",
    "\n",
    "    # batch size is tuned by your HyperbandWithBatchSize tuner\n",
    "    hp.Choice(\"batch_size\", [16, 32, 64, 128])\n",
    "\n",
    "    # --- depth ---\n",
    "    n_layers = hp.Int(\"n_layers\", 2, 6)\n",
    "\n",
    "    # --- regularization / optimizer ---\n",
    "    l2_strength = hp.Choice(\"l2\", [0.0, 1e-5, 1e-4, 1e-3])\n",
    "    lr = hp.Choice(\"lr\", [1e-4, 3e-4, 1e-3, 3e-3])\n",
    "\n",
    "    # --- dropout (global) ---\n",
    "    dropout = hp.Float(\"dropout\", 0.1, 0.6, step=0.1)\n",
    "\n",
    "    # --- units search space ---\n",
    "    units_grid = [8, 16, 32, 64, 128, 256]\n",
    "\n",
    "    # --- architecture mode ---\n",
    "    arch_mode = hp.Choice(\"arch_mode\", [\"free\", \"pyramid_half\"])\n",
    "\n",
    "    # Decide units per layer\n",
    "    layer_units = []\n",
    "\n",
    "    if arch_mode == \"free\":\n",
    "        for i in range(n_layers):\n",
    "            u = hp.Choice(f\"units_l{i+1}\", units_grid)\n",
    "            layer_units.append(u)\n",
    "\n",
    "    elif arch_mode == \"pyramid_half\":\n",
    "        base_units = hp.Choice(\"base_units\", units_grid)\n",
    "        min_units = hp.Choice(\"min_units\", [8, 16])\n",
    "\n",
    "        u = base_units\n",
    "        for i in range(n_layers):\n",
    "            layer_units.append(max(u, min_units))\n",
    "            u = u // 2\n",
    "\n",
    "    # --- build network ---\n",
    "    inputs = keras.Input(shape=(input_dim,), name=\"expression\")\n",
    "    x = inputs\n",
    "\n",
    "    for i, u in enumerate(layer_units, start=1):\n",
    "        x = layers.Dense(\n",
    "            units=u,\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=regularizers.l2(l2_strength),\n",
    "            name=f\"dense_{i}\",\n",
    "        )(x)\n",
    "\n",
    "        if dropout and dropout > 0:\n",
    "            x = layers.Dropout(dropout, name=f\"dropout_{i}\")(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"MLP_grade\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f1fa8",
   "metadata": {},
   "source": [
    "Step 7: We tune the hyper-paramaters to choose the best model and avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea6b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperbandWithBatchSize(kt.Hyperband):\n",
    "    \"\"\"Hyperband tuner that automatically uses hp['batch_size'] when fitting.\"\"\"\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        kwargs[\"batch_size\"] = hp.get(\"batch_size\")\n",
    "        return super().run_trial(trial, *args, **kwargs)\n",
    "\n",
    "def tune_hyperparameters(X_train_s, y_train, X_val_s, y_val, input_dim: int, run_dir: str, cfg: Config):\n",
    "    tuner_dir = os.path.join(run_dir, \"tuner\")\n",
    "    os.makedirs(tuner_dir, exist_ok=True)\n",
    "\n",
    "    tuner = HyperbandWithBatchSize(\n",
    "        hypermodel=lambda hp: build_model(hp, input_dim=input_dim, num_classes=3),\n",
    "        objective=kt.Objective(cfg.objective, direction=\"max\"),\n",
    "        max_epochs=cfg.max_epochs,\n",
    "        factor=cfg.hyperband_factor,\n",
    "        directory=tuner_dir,\n",
    "        project_name=\"grade_mlp\"\n",
    "    )\n",
    "\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        patience=cfg.early_stop_patience,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        X_train_s, y_train,\n",
    "        validation_data=(X_val_s, y_val),\n",
    "        epochs=cfg.max_epochs,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "    return tuner, best_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801fd97",
   "metadata": {},
   "source": [
    "Step 8: We choose the best model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6e18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_class_weight(y_train: np.ndarray) -> dict:\n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "    return {int(c): float(w) for c, w in zip(classes, weights)}\n",
    "\n",
    "def train_best(X_train_s, y_train, X_val_s, y_val, input_dim: int, best_hp, run_dir: str, cfg: Config):\n",
    "    model = build_model(best_hp, input_dim=input_dim, num_classes=3)\n",
    "\n",
    "    ckpt_path = os.path.join(run_dir, \"best_model.keras\")\n",
    "\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        patience=cfg.early_stop_patience,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=ckpt_path,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    # class weights (computed on this fold's training labels)\n",
    "    class_weight = make_class_weight(y_train)\n",
    "    print(\"Class weight:\", class_weight)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_s, y_train,\n",
    "        validation_data=(X_val_s, y_val),\n",
    "        epochs=cfg.max_epochs,\n",
    "        batch_size=best_hp.get(\"batch_size\"),\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1,\n",
    "        class_weight=class_weight  # NEW\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(run_dir, \"history.json\"), \"w\") as f:\n",
    "        json.dump(history.history, f, indent=2)\n",
    "\n",
    "    return model, ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef7714",
   "metadata": {},
   "source": [
    "Step 9: We evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbaed84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: keras.Model, X_eval_s, y_eval, run_dir: str, prefix: str = \"eval\"):\n",
    "    eval_loss, eval_acc = model.evaluate(X_eval_s, y_eval, verbose=0)\n",
    "\n",
    "    probs = model.predict(X_eval_s, verbose=0)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "    report = classification_report(y_eval, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_eval, y_pred).tolist()\n",
    "    macro_f1 = float(f1_score(y_eval, y_pred, average=\"macro\"))\n",
    "\n",
    "    results = {\n",
    "        \"loss\": float(eval_loss),\n",
    "        \"accuracy\": float(eval_acc),\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"classification_report\": report,\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(run_dir, f\"{prefix}_metrics.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6d782",
   "metadata": {},
   "source": [
    "Step 10: We save all the run metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f9916f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run_metadata(run_dir: str, cfg: Config, best_hp, scaler):\n",
    "    with open(os.path.join(run_dir, \"config.json\"), \"w\") as f:\n",
    "        json.dump(asdict(cfg), f, indent=2)\n",
    "\n",
    "    with open(os.path.join(run_dir, \"best_hyperparameters.json\"), \"w\") as f:\n",
    "        json.dump(best_hp.values, f, indent=2)\n",
    "\n",
    "    joblib.dump(scaler, os.path.join(run_dir, \"scaler.joblib\"))\n",
    "\n",
    "    versions = {\n",
    "        \"python\": f\"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}\",\n",
    "        \"numpy\": np.__version__,\n",
    "        \"pandas\": pd.__version__,\n",
    "        \"tensorflow\": tf.__version__,\n",
    "        \"keras_tuner\": kt.__version__\n",
    "    }\n",
    "    with open(os.path.join(run_dir, \"versions.json\"), \"w\") as f:\n",
    "        json.dump(versions, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb81ee",
   "metadata": {},
   "source": [
    "Step 11: We plot Evaluation Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2ffa5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def evaluate_and_save_plots(\n",
    "    model,\n",
    "    X_eval_s: np.ndarray,\n",
    "    y_eval: np.ndarray,\n",
    "    out_dir: str,\n",
    "    class_names=(\"G2\", \"G3\", \"G4\"),\n",
    "    prefix: str = \"test\",\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # ---- predictions ----\n",
    "    eval_loss, eval_acc = model.evaluate(X_eval_s, y_eval, verbose=0)\n",
    "    probs = model.predict(X_eval_s, verbose=0)          # (n, C)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "    # ---- metrics dict (same spirit as your evaluate()) ----\n",
    "    report = classification_report(y_eval, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(y_eval, y_pred)\n",
    "    macro_f1 = float(f1_score(y_eval, y_pred, average=\"macro\"))\n",
    "\n",
    "    results = {\n",
    "        \"loss\": float(eval_loss),\n",
    "        \"accuracy\": float(eval_acc),\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"classification_report\": report,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(out_dir, f\"{prefix}_metrics.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Confusion matrix (raw)\n",
    "    # ----------------------------\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm)\n",
    "    ax.set_xticks(range(n_classes))\n",
    "    ax.set_yticks(range(n_classes))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(class_names)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(f\"Confusion Matrix ({prefix})\")\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, f\"{prefix}_confusion_matrix.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Confusion matrix (row-normalized)\n",
    "    # ----------------------------\n",
    "    cm_norm = cm / np.clip(cm.sum(axis=1, keepdims=True), 1, None)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm_norm)\n",
    "    ax.set_xticks(range(n_classes))\n",
    "    ax.set_yticks(range(n_classes))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(class_names)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(f\"Confusion Matrix Normalized ({prefix})\")\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            ax.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, f\"{prefix}_cm_normalized.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Per-class precision/recall/F1 bar plot\n",
    "    # ----------------------------\n",
    "    rows = []\n",
    "    for i, name in enumerate(class_names):\n",
    "        d = report.get(str(i), None)\n",
    "        if d is None:\n",
    "            rows.append([name, np.nan, np.nan, np.nan])\n",
    "        else:\n",
    "            rows.append([name, d[\"precision\"], d[\"recall\"], d[\"f1-score\"]])\n",
    "\n",
    "    rows = np.array(rows, dtype=float, copy=False) if False else rows  # no-op; keeps notebook happy\n",
    "\n",
    "    precision = [r[1] for r in rows]\n",
    "    recall    = [r[2] for r in rows]\n",
    "    f1s       = [r[3] for r in rows]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(n_classes)\n",
    "    ax.bar(x - 0.2, precision, width=0.2, label=\"precision\")\n",
    "    ax.bar(x,       recall,    width=0.2, label=\"recall\")\n",
    "    ax.bar(x + 0.2, f1s,       width=0.2, label=\"f1\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names)\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.set_title(f\"Per-class metrics ({prefix})\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, f\"{prefix}_per_class_metrics.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) ROC curves (one-vs-rest)\n",
    "    # ----------------------------\n",
    "    Y = label_binarize(y_eval, classes=list(range(n_classes)))  # (n, C)\n",
    "\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for c in range(n_classes):\n",
    "        fpr[c], tpr[c], _ = roc_curve(Y[:, c], probs[:, c])\n",
    "        roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y.ravel(), probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for c, name in enumerate(class_names):\n",
    "        ax.plot(fpr[c], tpr[c], label=f\"{name} (AUC={roc_auc[c]:.3f})\")\n",
    "    ax.plot(fpr[\"micro\"], tpr[\"micro\"], linestyle=\"--\", label=f\"micro (AUC={roc_auc['micro']:.3f})\")\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\":\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(f\"ROC (one-vs-rest) ({prefix})\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, f\"{prefix}_roc_curves.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5) Precision–Recall curves (one-vs-rest)\n",
    "    # ----------------------------\n",
    "    pr, re, ap = {}, {}, {}\n",
    "    for c in range(n_classes):\n",
    "        pr[c], re[c], _ = precision_recall_curve(Y[:, c], probs[:, c])\n",
    "        ap[c] = average_precision_score(Y[:, c], probs[:, c])\n",
    "\n",
    "    pr_micro, re_micro, _ = precision_recall_curve(Y.ravel(), probs.ravel())\n",
    "    ap_micro = average_precision_score(Y, probs, average=\"micro\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for c, name in enumerate(class_names):\n",
    "        ax.plot(re[c], pr[c], label=f\"{name} (AP={ap[c]:.3f})\")\n",
    "    ax.plot(re_micro, pr_micro, linestyle=\"--\", label=f\"micro (AP={ap_micro:.3f})\")\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_title(f\"Precision–Recall (one-vs-rest) ({prefix})\")\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, f\"{prefix}_pr_curves.png\"), dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b6ba1",
   "metadata": {},
   "source": [
    "Step 12: Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94ebc84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_classes(model, X_s: np.ndarray, batch_size: int = 256) -> np.ndarray:\n",
    "    probs = model.predict(X_s, verbose=0, batch_size=batch_size)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "def _compute_metrics(y_true, y_pred, n_classes: int):\n",
    "    out = {}\n",
    "    out[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    out[\"macro_f1\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    out[\"per_class_f1\"] = f1_score(y_true, y_pred, average=None, labels=list(range(n_classes)))\n",
    "    out[\"per_class_recall\"] = recall_score(y_true, y_pred, average=None, labels=list(range(n_classes)))\n",
    "    return out\n",
    "\n",
    "def permutation_importance_multiclass(\n",
    "    model,\n",
    "    X_s: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    feature_names,\n",
    "    out_dir: str,\n",
    "    class_names=(\"G2\", \"G3\", \"G4\"),\n",
    "    n_repeats: int = 5,\n",
    "    top_k: int = 10,\n",
    "    max_features: int | None = 200,     # IMPORTANT: change to None to run all (can be very slow)\n",
    "    batch_size_pred: int = 256,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Permutation importance on a fixed eval set (e.g. held-out test).\n",
    "\n",
    "    Global metrics:\n",
    "      - accuracy\n",
    "      - macro_f1\n",
    "\n",
    "    Per-class metrics:\n",
    "      - f1 per class\n",
    "      - recall per class (acts like per-class accuracy)\n",
    "\n",
    "    Saves:\n",
    "      - CSVs with mean +/- std drops\n",
    "      - Top-K bar plots for each metric\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    X_s = np.asarray(X_s)\n",
    "    y = np.asarray(y).astype(int)\n",
    "    n_classes = len(class_names)\n",
    "    n_samples, n_features = X_s.shape\n",
    "\n",
    "    feature_names = np.array(list(feature_names), dtype=object)\n",
    "    if feature_names.shape[0] != n_features:\n",
    "        raise ValueError(\"feature_names length does not match X_s number of columns.\")\n",
    "\n",
    "    # --- choose features to evaluate ---\n",
    "    if (max_features is not None) and (n_features > max_features):\n",
    "        # pick highest-variance features (simple, fast, reasonable default)\n",
    "        variances = X_s.var(axis=0)\n",
    "        feat_idx = np.argsort(-variances)[:max_features]\n",
    "        feat_idx = np.sort(feat_idx)\n",
    "        note = f\"NOTE: computed permutation importance on top-{max_features} variance genes (out of {n_features}).\"\n",
    "    else:\n",
    "        feat_idx = np.arange(n_features)\n",
    "        note = f\"NOTE: computed permutation importance on ALL genes ({n_features}).\"\n",
    "\n",
    "    with open(os.path.join(out_dir, \"perm_importance_NOTE.txt\"), \"w\") as f:\n",
    "        f.write(note + \"\\n\")\n",
    "    print(note)\n",
    "\n",
    "    # --- baseline predictions/metrics ---\n",
    "    y_pred_base = _predict_classes(model, X_s, batch_size=batch_size_pred)\n",
    "    base = _compute_metrics(y, y_pred_base, n_classes)\n",
    "\n",
    "    # storage: drops (baseline - permuted) so higher drop => more important\n",
    "    drops_acc = np.zeros((len(feat_idx), n_repeats), dtype=float)\n",
    "    drops_macro_f1 = np.zeros((len(feat_idx), n_repeats), dtype=float)\n",
    "    drops_f1 = np.zeros((len(feat_idx), n_repeats, n_classes), dtype=float)\n",
    "    drops_recall = np.zeros((len(feat_idx), n_repeats, n_classes), dtype=float)\n",
    "\n",
    "    X_work = X_s.copy()  # will permute columns in-place, then restore\n",
    "\n",
    "    for fi_pos, j in enumerate(feat_idx):\n",
    "        original_col = X_work[:, j].copy()\n",
    "\n",
    "        for r in range(n_repeats):\n",
    "            perm = rng.permutation(n_samples)\n",
    "            X_work[:, j] = original_col[perm]\n",
    "\n",
    "            y_pred_perm = _predict_classes(model, X_work, batch_size=batch_size_pred)\n",
    "            m = _compute_metrics(y, y_pred_perm, n_classes)\n",
    "\n",
    "            drops_acc[fi_pos, r] = base[\"accuracy\"] - m[\"accuracy\"]\n",
    "            drops_macro_f1[fi_pos, r] = base[\"macro_f1\"] - m[\"macro_f1\"]\n",
    "            drops_f1[fi_pos, r, :] = base[\"per_class_f1\"] - m[\"per_class_f1\"]\n",
    "            drops_recall[fi_pos, r, :] = base[\"per_class_recall\"] - m[\"per_class_recall\"]\n",
    "\n",
    "        # restore column\n",
    "        X_work[:, j] = original_col\n",
    "\n",
    "    # --- summarize ---\n",
    "    names_sel = feature_names[feat_idx]\n",
    "\n",
    "    df_global = pd.DataFrame({\n",
    "        \"feature\": names_sel,\n",
    "        \"drop_accuracy_mean\": drops_acc.mean(axis=1),\n",
    "        \"drop_accuracy_std\": drops_acc.std(axis=1),\n",
    "        \"drop_macro_f1_mean\": drops_macro_f1.mean(axis=1),\n",
    "        \"drop_macro_f1_std\": drops_macro_f1.std(axis=1),\n",
    "    }).sort_values(\"drop_macro_f1_mean\", ascending=False)\n",
    "\n",
    "    # per-class tables\n",
    "    per_class_rows = []\n",
    "    for c, cname in enumerate(class_names):\n",
    "        tmp = pd.DataFrame({\n",
    "            \"feature\": names_sel,\n",
    "            f\"drop_f1_{cname}_mean\": drops_f1[:, :, c].mean(axis=1),\n",
    "            f\"drop_f1_{cname}_std\": drops_f1[:, :, c].std(axis=1),\n",
    "            f\"drop_recall_{cname}_mean\": drops_recall[:, :, c].mean(axis=1),\n",
    "            f\"drop_recall_{cname}_std\": drops_recall[:, :, c].std(axis=1),\n",
    "        })\n",
    "        per_class_rows.append(tmp)\n",
    "\n",
    "    df_per_class = per_class_rows[0]\n",
    "    for k in per_class_rows[1:]:\n",
    "        df_per_class = df_per_class.merge(k, on=\"feature\")\n",
    "\n",
    "    # save csvs\n",
    "    df_global.to_csv(os.path.join(out_dir, \"perm_importance_global.csv\"), index=False)\n",
    "    df_per_class.to_csv(os.path.join(out_dir, \"perm_importance_per_class.csv\"), index=False)\n",
    "\n",
    "    # --- plotting helpers ---\n",
    "    def _bar_topk(df, score_col, std_col, title, filename, top_k=10):\n",
    "        d = df.sort_values(score_col, ascending=False).head(top_k).copy()\n",
    "        d = d.iloc[::-1]  # so top is at top in barh\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 0.4*len(d) + 2.5))\n",
    "        ax.barh(d[\"feature\"].astype(str), d[score_col].to_numpy(), xerr=d[std_col].to_numpy())\n",
    "        ax.set_xlabel(\"Performance drop after permutation\")\n",
    "        ax.set_title(title)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(out_dir, filename), dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # global plots\n",
    "    _bar_topk(\n",
    "        df_global, \"drop_accuracy_mean\", \"drop_accuracy_std\",\n",
    "        title=f\"Permutation importance (Global Accuracy) — Top {top_k}\",\n",
    "        filename=f\"perm_top{top_k}_global_accuracy.png\",\n",
    "        top_k=top_k\n",
    "    )\n",
    "    _bar_topk(\n",
    "        df_global, \"drop_macro_f1_mean\", \"drop_macro_f1_std\",\n",
    "        title=f\"Permutation importance (Global Macro-F1) — Top {top_k}\",\n",
    "        filename=f\"perm_top{top_k}_global_macro_f1.png\",\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    # per-class plots (F1 + Recall)\n",
    "    for cname in class_names:\n",
    "        _bar_topk(\n",
    "            df_per_class,\n",
    "            f\"drop_f1_{cname}_mean\", f\"drop_f1_{cname}_std\",\n",
    "            title=f\"Permutation importance (Per-class F1: {cname}) — Top {top_k}\",\n",
    "            filename=f\"perm_top{top_k}_f1_{cname}.png\",\n",
    "            top_k=top_k\n",
    "        )\n",
    "        _bar_topk(\n",
    "            df_per_class,\n",
    "            f\"drop_recall_{cname}_mean\", f\"drop_recall_{cname}_std\",\n",
    "            title=f\"Permutation importance (Per-class Recall: {cname}) — Top {top_k}\",\n",
    "            filename=f\"perm_top{top_k}_recall_{cname}.png\",\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "    # return dataframes + baseline metrics for reporting\n",
    "    baseline_summary = {\n",
    "        \"baseline_accuracy\": float(base[\"accuracy\"]),\n",
    "        \"baseline_macro_f1\": float(base[\"macro_f1\"]),\n",
    "        \"baseline_per_class_f1\": {class_names[i]: float(base[\"per_class_f1\"][i]) for i in range(n_classes)},\n",
    "        \"baseline_per_class_recall\": {class_names[i]: float(base[\"per_class_recall\"][i]) for i in range(n_classes)},\n",
    "        \"note\": note,\n",
    "        \"n_repeats\": int(n_repeats),\n",
    "        \"evaluated_features\": int(len(feat_idx)),\n",
    "    }\n",
    "    with open(os.path.join(out_dir, \"perm_importance_baseline.json\"), \"w\") as f:\n",
    "        import json\n",
    "        json.dump(baseline_summary, f, indent=2)\n",
    "\n",
    "    print(\"Saved permutation importance CSVs + plots to:\", out_dir)\n",
    "    return df_global, df_per_class, baseline_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3c5db",
   "metadata": {},
   "source": [
    "Final Step: Main run script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75397e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(expression_sup: pd.DataFrame, samples_sup: pd.DataFrame):\n",
    "    cfg = Config()\n",
    "    if cfg.label_map is None:\n",
    "        cfg.label_map = {\"G2\": 0, \"G3\": 1, \"G4\": 2}\n",
    "\n",
    "    set_global_seed(cfg.seed)\n",
    "    run_dir = make_run_dir(cfg.out_dir)\n",
    "\n",
    "    # 1) labels\n",
    "    y = make_y(samples_sup, cfg.label_col, cfg.label_map)\n",
    "\n",
    "    # 2) hold-out test\n",
    "    X_trainval, X_test, y_trainval, y_test = split_holdout_test(expression_sup, y, cfg)\n",
    "    save_test_ids(run_dir, X_test)\n",
    "\n",
    "    # 3) 9-fold CV on trainval\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)\n",
    "\n",
    "    cv_results = []\n",
    "    best_hp = None\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_trainval, y_trainval), start=1):\n",
    "        fold_dir = os.path.join(run_dir, f\"fold_{fold:02d}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "        X_tr = X_trainval.iloc[tr_idx]\n",
    "        X_va = X_trainval.iloc[va_idx]\n",
    "        y_tr = y_trainval[tr_idx]\n",
    "        y_va = y_trainval[va_idx]\n",
    "\n",
    "        save_fold_ids(run_dir, fold, X_tr, X_va)\n",
    "\n",
    "        # standardize per-fold \n",
    "        X_tr_s, X_va_s, _scaler_fold = standardize_train_val(X_tr, X_va)\n",
    "\n",
    "        # tune once and reuse HP for all folds to save time\n",
    "        if best_hp is None:\n",
    "            tuner, best_hp = tune_hyperparameters(\n",
    "                X_tr_s, y_tr,\n",
    "                X_va_s, y_va,\n",
    "                input_dim=X_tr_s.shape[1],\n",
    "                run_dir=fold_dir,\n",
    "                cfg=cfg\n",
    "            )\n",
    "\n",
    "            with open(os.path.join(run_dir, \"best_hyperparameters.json\"), \"w\") as f:\n",
    "                json.dump(best_hp.values, f, indent=2)\n",
    "\n",
    "        # train best HP on this fold\n",
    "        model, ckpt_path = train_best(\n",
    "            X_tr_s, y_tr,\n",
    "            X_va_s, y_va,\n",
    "            input_dim=X_tr_s.shape[1],\n",
    "            best_hp=best_hp,\n",
    "            run_dir=fold_dir,\n",
    "            cfg=cfg\n",
    "        )\n",
    "\n",
    "        # evaluate on fold validation\n",
    "        fold_metrics = evaluate(model, X_va_s, y_va, fold_dir, prefix=\"val\")\n",
    "\n",
    "        cv_results.append({\n",
    "            \"fold\": fold,\n",
    "            \"val_accuracy\": fold_metrics[\"accuracy\"],\n",
    "            \"val_macro_f1\": fold_metrics[\"macro_f1\"],\n",
    "            \"model_path\": ckpt_path\n",
    "        })\n",
    "\n",
    "    with open(os.path.join(run_dir, \"cv_results.json\"), \"w\") as f:\n",
    "        json.dump(cv_results, f, indent=2)\n",
    "\n",
    "    # 4) final training on all trainval with a small internal val split for early stopping\n",
    "    X_tr_final, X_va_final, y_tr_final, y_va_final = train_test_split(\n",
    "        X_trainval, y_trainval,\n",
    "        test_size=0.15,\n",
    "        random_state=cfg.seed,\n",
    "        stratify=y_trainval\n",
    "    )\n",
    "\n",
    "    scaler_final = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_tr_final_s = scaler_final.fit_transform(X_tr_final).astype(np.float32)\n",
    "    X_va_final_s = scaler_final.transform(X_va_final).astype(np.float32)\n",
    "    X_test_s = scaler_final.transform(X_test).astype(np.float32)\n",
    "\n",
    "    final_dir = os.path.join(run_dir, \"final\")\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "    model_final, ckpt_path_final = train_best(\n",
    "        X_tr_final_s, y_tr_final,\n",
    "        X_va_final_s, y_va_final,\n",
    "        input_dim=X_tr_final_s.shape[1],\n",
    "        best_hp=best_hp,\n",
    "        run_dir=final_dir,\n",
    "        cfg=cfg\n",
    "    )\n",
    "\n",
    "    #feature importance on final model\n",
    "    df_global, df_per_class, baseline_pi = permutation_importance_multiclass(\n",
    "    model=model_final,\n",
    "    X_s=X_test_s,\n",
    "    y=y_test,\n",
    "    feature_names=X_test.columns,      # gene IDs in correct order\n",
    "    out_dir=final_dir,                 # save next to your other plots\n",
    "    class_names=(\"G2\", \"G3\", \"G4\"),\n",
    "    n_repeats=5,\n",
    "    top_k=10,\n",
    "    max_features=200,                  # IMPORTANT: increase if you can afford it\n",
    "    batch_size_pred=256,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "\n",
    "    # 5) evaluate once on held-out test\n",
    "    class_names = [\"G2\", \"G3\", \"G4\"]\n",
    "    test_results = evaluate_and_save_plots(\n",
    "    model_final,\n",
    "    X_test_s,\n",
    "    y_test,\n",
    "    out_dir=final_dir,     \n",
    "    class_names=class_names,\n",
    "    prefix=\"test\",\n",
    "    )\n",
    "    # 6) save metadata + final scaler\n",
    "    save_run_metadata(final_dir, cfg, best_hp, scaler_final)\n",
    "\n",
    "    # print summary\n",
    "    val_accs = [r[\"val_accuracy\"] for r in cv_results]\n",
    "    val_f1s = [r[\"val_macro_f1\"] for r in cv_results]\n",
    "\n",
    "    print(\"Saved run to:\", run_dir)\n",
    "    print(\"Final model:\", ckpt_path_final)\n",
    "    print(f\"CV val accuracy: mean={np.mean(val_accs):.4f} std={np.std(val_accs):.4f}\")\n",
    "    print(f\"CV val macro F1: mean={np.mean(val_f1s):.4f} std={np.std(val_f1s):.4f}\")\n",
    "    print(\"Test accuracy:\", test_results[\"accuracy\"])\n",
    "    print(\"Test macro F1:\", test_results[\"macro_f1\"])\n",
    "\n",
    "    return test_results, run_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a633a18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 00m 17s]\n",
      "val_accuracy: 0.7051281929016113\n",
      "\n",
      "Best val_accuracy So Far: 0.8589743375778198\n",
      "Total elapsed time: 00h 49m 16s\n",
      "Class weight: {0: 1.2935010482180294, 1: 1.261758691206544, 2: 0.6971751412429379}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7293 - loss: 0.9049 - val_accuracy: 0.7179 - val_loss: 0.5916\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8298 - loss: 0.4919 - val_accuracy: 0.7949 - val_loss: 0.4891\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8460 - loss: 0.4206 - val_accuracy: 0.7564 - val_loss: 0.5025\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8768 - loss: 0.3406 - val_accuracy: 0.7436 - val_loss: 0.4952\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9141 - loss: 0.2731 - val_accuracy: 0.7821 - val_loss: 0.6887\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9173 - loss: 0.2539 - val_accuracy: 0.7692 - val_loss: 0.6804\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9190 - loss: 0.2384 - val_accuracy: 0.7821 - val_loss: 0.5575\n",
      "Class weight: {0: 1.2935010482180294, 1: 1.261758691206544, 2: 0.6971751412429379}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7504 - loss: 0.7580 - val_accuracy: 0.7821 - val_loss: 0.4500\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8460 - loss: 0.5138 - val_accuracy: 0.8462 - val_loss: 0.3740\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8476 - loss: 0.3946 - val_accuracy: 0.8205 - val_loss: 0.4217\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8622 - loss: 0.3693 - val_accuracy: 0.8205 - val_loss: 0.4444\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8817 - loss: 0.3136 - val_accuracy: 0.8462 - val_loss: 0.4682\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8914 - loss: 0.2951 - val_accuracy: 0.8462 - val_loss: 0.4109\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8963 - loss: 0.3021 - val_accuracy: 0.8590 - val_loss: 0.4186\n",
      "Epoch 8/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8947 - loss: 0.2692 - val_accuracy: 0.8077 - val_loss: 0.3846\n",
      "Epoch 9/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9352 - loss: 0.1871 - val_accuracy: 0.8846 - val_loss: 0.4237\n",
      "Epoch 10/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9498 - loss: 0.1713 - val_accuracy: 0.8846 - val_loss: 0.3769\n",
      "Epoch 11/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9643 - loss: 0.1248 - val_accuracy: 0.8718 - val_loss: 0.4319\n",
      "Epoch 12/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9449 - loss: 0.1525 - val_accuracy: 0.8590 - val_loss: 0.3629\n",
      "Epoch 13/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9676 - loss: 0.1409 - val_accuracy: 0.8718 - val_loss: 0.4537\n",
      "Epoch 14/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9692 - loss: 0.0977 - val_accuracy: 0.8462 - val_loss: 0.6174\n",
      "Class weight: {0: 1.2955974842767295, 1: 1.2638036809815951, 2: 0.6959459459459459}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7249 - loss: 0.8225 - val_accuracy: 0.7662 - val_loss: 0.4487\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7994 - loss: 0.5351 - val_accuracy: 0.8312 - val_loss: 0.4715\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8625 - loss: 0.3672 - val_accuracy: 0.8571 - val_loss: 0.3952\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8932 - loss: 0.2982 - val_accuracy: 0.8312 - val_loss: 0.4993\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8900 - loss: 0.3104 - val_accuracy: 0.7792 - val_loss: 0.6093\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8900 - loss: 0.3158 - val_accuracy: 0.8312 - val_loss: 0.6110\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8997 - loss: 0.2750 - val_accuracy: 0.8701 - val_loss: 0.5103\n",
      "Epoch 8/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9256 - loss: 0.2120 - val_accuracy: 0.8442 - val_loss: 0.5463\n",
      "Epoch 9/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9385 - loss: 0.2057 - val_accuracy: 0.8831 - val_loss: 0.5037\n",
      "Epoch 10/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9353 - loss: 0.1850 - val_accuracy: 0.8312 - val_loss: 0.5451\n",
      "Epoch 11/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9401 - loss: 0.1773 - val_accuracy: 0.8571 - val_loss: 0.5439\n",
      "Epoch 12/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9385 - loss: 0.1856 - val_accuracy: 0.8571 - val_loss: 0.4913\n",
      "Epoch 13/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9417 - loss: 0.1937 - val_accuracy: 0.8312 - val_loss: 0.5032\n",
      "Epoch 14/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9417 - loss: 0.1576 - val_accuracy: 0.8442 - val_loss: 0.5543\n",
      "Class weight: {0: 1.2955974842767295, 1: 1.2560975609756098, 2: 0.6983050847457627}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7023 - loss: 0.7710 - val_accuracy: 0.7922 - val_loss: 0.4905\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8058 - loss: 0.5055 - val_accuracy: 0.7143 - val_loss: 0.5626\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8706 - loss: 0.3733 - val_accuracy: 0.7922 - val_loss: 0.5380\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8738 - loss: 0.3451 - val_accuracy: 0.8312 - val_loss: 0.4653\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8738 - loss: 0.3526 - val_accuracy: 0.8052 - val_loss: 0.4173\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9094 - loss: 0.2479 - val_accuracy: 0.7922 - val_loss: 0.3642\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9029 - loss: 0.3013 - val_accuracy: 0.8052 - val_loss: 0.4646\n",
      "Epoch 8/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9466 - loss: 0.1919 - val_accuracy: 0.8182 - val_loss: 0.3621\n",
      "Epoch 9/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9482 - loss: 0.1683 - val_accuracy: 0.8312 - val_loss: 0.4464\n",
      "Class weight: {0: 1.2955974842767295, 1: 1.2560975609756098, 2: 0.6983050847457627}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7201 - loss: 0.8769 - val_accuracy: 0.7792 - val_loss: 0.4062\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7961 - loss: 0.4793 - val_accuracy: 0.8312 - val_loss: 0.3411\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8430 - loss: 0.4242 - val_accuracy: 0.8701 - val_loss: 0.2966\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8495 - loss: 0.4321 - val_accuracy: 0.8571 - val_loss: 0.3947\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8803 - loss: 0.3086 - val_accuracy: 0.8182 - val_loss: 0.3322\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8997 - loss: 0.2795 - val_accuracy: 0.8701 - val_loss: 0.3454\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8981 - loss: 0.2546 - val_accuracy: 0.8571 - val_loss: 0.2675\n",
      "Epoch 8/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9288 - loss: 0.2094 - val_accuracy: 0.8831 - val_loss: 0.3131\n",
      "Epoch 9/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9304 - loss: 0.1934 - val_accuracy: 0.8571 - val_loss: 0.3984\n",
      "Epoch 10/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9547 - loss: 0.1633 - val_accuracy: 0.8831 - val_loss: 0.2989\n",
      "Epoch 11/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9466 - loss: 0.1598 - val_accuracy: 0.8312 - val_loss: 0.4226\n",
      "Epoch 12/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9531 - loss: 0.1423 - val_accuracy: 0.9091 - val_loss: 0.3598\n",
      "Epoch 13/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9660 - loss: 0.1374 - val_accuracy: 0.8701 - val_loss: 0.3298\n",
      "Epoch 14/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9515 - loss: 0.1542 - val_accuracy: 0.8831 - val_loss: 0.2668\n",
      "Epoch 15/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9595 - loss: 0.1404 - val_accuracy: 0.8571 - val_loss: 0.4750\n",
      "Epoch 16/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9628 - loss: 0.1618 - val_accuracy: 0.8961 - val_loss: 0.3255\n",
      "Epoch 17/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9725 - loss: 0.1074 - val_accuracy: 0.8701 - val_loss: 0.3769\n",
      "Class weight: {0: 1.2955974842767295, 1: 1.2560975609756098, 2: 0.6983050847457627}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7039 - loss: 0.9059 - val_accuracy: 0.7662 - val_loss: 0.5164\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8301 - loss: 0.4716 - val_accuracy: 0.8182 - val_loss: 0.4730\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8560 - loss: 0.3744 - val_accuracy: 0.8182 - val_loss: 0.5025\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8673 - loss: 0.3477 - val_accuracy: 0.8182 - val_loss: 0.5062\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9045 - loss: 0.2568 - val_accuracy: 0.8442 - val_loss: 0.4309\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9337 - loss: 0.2325 - val_accuracy: 0.8701 - val_loss: 0.5936\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9272 - loss: 0.2399 - val_accuracy: 0.8442 - val_loss: 0.4631\n",
      "Epoch 8/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9369 - loss: 0.2078 - val_accuracy: 0.8052 - val_loss: 0.4865\n",
      "Epoch 9/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9466 - loss: 0.1720 - val_accuracy: 0.7922 - val_loss: 0.5968\n",
      "Epoch 10/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9595 - loss: 0.1384 - val_accuracy: 0.8312 - val_loss: 0.5830\n",
      "Epoch 11/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9612 - loss: 0.1357 - val_accuracy: 0.8182 - val_loss: 0.5882\n",
      "Class weight: {0: 1.2955974842767295, 1: 1.2560975609756098, 2: 0.6983050847457627}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7184 - loss: 0.8081 - val_accuracy: 0.8182 - val_loss: 0.3675\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8204 - loss: 0.4507 - val_accuracy: 0.8182 - val_loss: 0.4487\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8722 - loss: 0.3421 - val_accuracy: 0.7662 - val_loss: 0.7783\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8722 - loss: 0.3325 - val_accuracy: 0.7792 - val_loss: 0.4815\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8948 - loss: 0.3130 - val_accuracy: 0.8442 - val_loss: 0.4176\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8981 - loss: 0.3030 - val_accuracy: 0.8442 - val_loss: 0.4530\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9207 - loss: 0.2216 - val_accuracy: 0.8052 - val_loss: 0.5184\n",
      "Epoch 8/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9288 - loss: 0.2018 - val_accuracy: 0.8182 - val_loss: 0.4531\n",
      "Epoch 9/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9466 - loss: 0.1629 - val_accuracy: 0.8182 - val_loss: 0.6003\n",
      "Epoch 10/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9563 - loss: 0.1464 - val_accuracy: 0.8182 - val_loss: 0.5573\n",
      "Class weight: {0: 1.2955974842767295, 1: 1.2560975609756098, 2: 0.6983050847457627}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7249 - loss: 0.8201 - val_accuracy: 0.8571 - val_loss: 0.3412\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8317 - loss: 0.4572 - val_accuracy: 0.8571 - val_loss: 0.3346\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8770 - loss: 0.3568 - val_accuracy: 0.8571 - val_loss: 0.3210\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8770 - loss: 0.3413 - val_accuracy: 0.8312 - val_loss: 0.3599\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8932 - loss: 0.2967 - val_accuracy: 0.8442 - val_loss: 0.3959\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8819 - loss: 0.3076 - val_accuracy: 0.8442 - val_loss: 0.3094\n",
      "Class weight: {0: 1.2875, 1: 1.2638036809815951, 2: 0.6983050847457627}\n",
      "Epoch 1/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7039 - loss: 0.8401 - val_accuracy: 0.7403 - val_loss: 0.6580\n",
      "Epoch 2/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7783 - loss: 0.5832 - val_accuracy: 0.7792 - val_loss: 0.5240\n",
      "Epoch 3/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8398 - loss: 0.4196 - val_accuracy: 0.7792 - val_loss: 0.5051\n",
      "Epoch 4/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8722 - loss: 0.3564 - val_accuracy: 0.8312 - val_loss: 0.4896\n",
      "Epoch 5/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9045 - loss: 0.2774 - val_accuracy: 0.7662 - val_loss: 0.5514\n",
      "Epoch 6/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8932 - loss: 0.2937 - val_accuracy: 0.7922 - val_loss: 0.4812\n",
      "Epoch 7/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9013 - loss: 0.2608 - val_accuracy: 0.7792 - val_loss: 0.5534\n",
      "Epoch 8/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9159 - loss: 0.2218 - val_accuracy: 0.8182 - val_loss: 0.5327\n",
      "Epoch 9/100\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9466 - loss: 0.1824 - val_accuracy: 0.7662 - val_loss: 0.6693\n",
      "Class weight: {0: 1.293859649122807, 1: 1.2606837606837606, 2: 0.6973995271867612}\n",
      "Epoch 1/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7271 - loss: 0.8811 - val_accuracy: 0.8381 - val_loss: 0.3640\n",
      "Epoch 2/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8203 - loss: 0.5795 - val_accuracy: 0.8095 - val_loss: 0.3466\n",
      "Epoch 3/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8458 - loss: 0.4192 - val_accuracy: 0.8381 - val_loss: 0.3382\n",
      "Epoch 4/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8593 - loss: 0.3555 - val_accuracy: 0.8667 - val_loss: 0.3065\n",
      "Epoch 5/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8949 - loss: 0.3191 - val_accuracy: 0.8571 - val_loss: 0.3296\n",
      "Epoch 6/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8898 - loss: 0.3200 - val_accuracy: 0.8190 - val_loss: 0.3368\n",
      "Epoch 7/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8814 - loss: 0.3346 - val_accuracy: 0.8476 - val_loss: 0.3552\n",
      "Epoch 8/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9068 - loss: 0.2799 - val_accuracy: 0.8762 - val_loss: 0.2935\n",
      "Epoch 9/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9288 - loss: 0.2333 - val_accuracy: 0.8476 - val_loss: 0.3995\n",
      "Epoch 10/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9305 - loss: 0.2017 - val_accuracy: 0.8762 - val_loss: 0.3165\n",
      "Epoch 11/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9373 - loss: 0.1838 - val_accuracy: 0.8667 - val_loss: 0.3003\n",
      "Epoch 12/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9407 - loss: 0.1740 - val_accuracy: 0.9143 - val_loss: 0.2577\n",
      "Epoch 13/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9390 - loss: 0.1873 - val_accuracy: 0.8762 - val_loss: 0.2773\n",
      "Epoch 14/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9458 - loss: 0.1774 - val_accuracy: 0.8476 - val_loss: 0.5247\n",
      "Epoch 15/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9525 - loss: 0.1293 - val_accuracy: 0.8762 - val_loss: 0.5008\n",
      "Epoch 16/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9407 - loss: 0.1767 - val_accuracy: 0.8762 - val_loss: 0.4831\n",
      "Epoch 17/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9593 - loss: 0.1501 - val_accuracy: 0.9048 - val_loss: 0.3523\n",
      "NOTE: computed permutation importance on top-200 variance genes (out of 2000).\n",
      "Saved permutation importance CSVs + plots to: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run\\20260105_025937\\final\n",
      "Saved run to: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run\\20260105_025937\n",
      "Final model: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run\\20260105_025937\\final\\best_model.keras\n",
      "CV val accuracy: mean=0.8562 std=0.0328\n",
      "CV val macro F1: mean=0.8192 std=0.0436\n",
      "Test accuracy: 0.8130081295967102\n",
      "Test macro F1: 0.7623981054960934\n"
     ]
    }
   ],
   "source": [
    "#Now we call the main function to run the entire pipeline\n",
    "results, run_dir = main(expression_sup, samples_sup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ded4a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MLP_grade\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"MLP_grade\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ expression (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">256,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ expression (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m256,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ softmax (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">793,739</span> (3.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m793,739\u001b[0m (3.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">264,579</span> (1.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m264,579\u001b[0m (1.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">529,160</span> (2.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m529,160\u001b[0m (2.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#look at the best model \n",
    "run_dir = r\"C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\"\n",
    "model_path = os.path.join(run_dir, \"final\", \"best_model.keras\")\n",
    "\n",
    "model = load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "219d85ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'n_layers': 2,\n",
       " 'l2': 1e-05,\n",
       " 'lr': 0.001,\n",
       " 'dropout': 0.1,\n",
       " 'arch_mode': 'pyramid_half',\n",
       " 'units_l1': 8,\n",
       " 'units_l2': 8,\n",
       " 'base_units': 128,\n",
       " 'min_units': 8,\n",
       " 'units_l3': 64,\n",
       " 'units_l4': 16,\n",
       " 'units_l5': 256,\n",
       " 'units_l6': 256,\n",
       " 'tuner/epochs': 12,\n",
       " 'tuner/initial_epoch': 4,\n",
       " 'tuner/bracket': 4,\n",
       " 'tuner/round': 2,\n",
       " 'tuner/trial_id': '0116'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_path = os.path.join(run_dir, \"best_hyperparameters.json\")\n",
    "with open(hp_path, \"r\") as f:\n",
    "    best_hp = json.load(f)\n",
    "\n",
    "best_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24848cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to identify G2/G3 cluster errors\n",
    "\n",
    "def load_ids_safely(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    ids = df.iloc[:, 0].astype(str).tolist()\n",
    "    ids = [i for i in ids if i not in (\"rownames\", \"index\", \"0\", \"None\") and i == i]\n",
    "    return ids\n",
    "\n",
    "def analyze_g2g3_cluster_errors(\n",
    "    run_dir: str,\n",
    "    expression_sup: pd.DataFrame,\n",
    "    samples_sup: pd.DataFrame,\n",
    "    label_col: str = \"tumor_grade\",\n",
    "    label_map: dict = None,\n",
    "    cluster_cols: list = None,\n",
    "):\n",
    "    if label_map is None:\n",
    "        label_map = {\"G2\": 0, \"G3\": 1, \"G4\": 2}\n",
    "\n",
    "    if cluster_cols is None:\n",
    "        cluster_cols = [\n",
    "            \"paper_Supervised.DNA.Methylation.Cluster\",\n",
    "            \"paper_IDH.specific.RNA.Expression.Cluster\",\n",
    "            \"paper_Pan.Glioma.DNA.Methylation.Cluster\",\n",
    "            \"paper_IDH.specific.DNA.Methylation.Cluster\",\n",
    "        ]\n",
    "\n",
    "    final_dir = os.path.join(run_dir, \"final\")\n",
    "    model_path = os.path.join(final_dir, \"best_model.keras\")\n",
    "    scaler_path = os.path.join(final_dir, \"scaler.joblib\")\n",
    "    test_ids_path = os.path.join(run_dir, \"test_ids.csv\")\n",
    "\n",
    "    model = load_model(model_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "\n",
    "    # ---- id loading ----\n",
    "    test_ids = load_ids_safely(test_ids_path)\n",
    "\n",
    "    # keep only ids that exist in expression_sup\n",
    "    test_ids = [i for i in test_ids if i in expression_sup.index]\n",
    "\n",
    "    if len(test_ids) == 0:\n",
    "        raise ValueError(\"No valid test IDs matched expression_sup.index. Check test_ids.csv format.\")\n",
    "\n",
    "    X_test = expression_sup.loc[test_ids]\n",
    "\n",
    "    y_all = samples_sup[label_col].map(label_map)\n",
    "    y_test = y_all.loc[X_test.index].astype(int).to_numpy()\n",
    "\n",
    "    X_test_s = scaler.transform(X_test).astype(np.float32)\n",
    "    probs = model.predict(X_test_s, verbose=0)\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "\n",
    "    cluster_cols = [c for c in cluster_cols if c in samples_sup.columns]\n",
    "    if len(cluster_cols) == 0:\n",
    "        raise ValueError(\"None of the requested cluster_cols exist in samples_sup.\")\n",
    "\n",
    "    pred_df = pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred}, index=X_test.index)\n",
    "    pred_df = pred_df.join(samples_sup.loc[pred_df.index, cluster_cols])\n",
    "\n",
    "    g23 = pred_df[pred_df[\"y_true\"].isin([0, 1])].copy()\n",
    "    g23[\"error_type\"] = np.where(\n",
    "        g23[\"y_true\"] == g23[\"y_pred\"],\n",
    "        \"correct\",\n",
    "        np.where((g23[\"y_true\"] == 0) & (g23[\"y_pred\"] == 1), \"G2→G3\",\n",
    "        np.where((g23[\"y_true\"] == 1) & (g23[\"y_pred\"] == 0), \"G3→G2\", \"other\"))\n",
    "    )\n",
    "    g23[\"wrong_g2g3\"] = g23[\"error_type\"].isin([\"G2→G3\", \"G3→G2\"])\n",
    "\n",
    "    print(\"G2/G3 confusion counts:\")\n",
    "    print(g23[\"error_type\"].value_counts(dropna=False))\n",
    "\n",
    "    for col in cluster_cols:\n",
    "        s = g23[col].astype(\"string\").fillna(\"NA\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Cluster column: {col}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        counts = pd.crosstab(s, g23[\"wrong_g2g3\"])\n",
    "        print(\"\\nCounts (False=correct, True=G2<->G3 confused):\")\n",
    "        print(counts)\n",
    "\n",
    "        rates = pd.crosstab(s, g23[\"wrong_g2g3\"], normalize=\"index\")\n",
    "        rates = rates.reindex(columns=[False, True], fill_value=0.0)\n",
    "        rates.columns = [\"correct_rate\", \"g2g3_confusion_rate\"]\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\nPer-cluster rates:\")\n",
    "        print(rates)\n",
    "\n",
    "        dir_tab = pd.crosstab(s, g23[\"error_type\"])\n",
    "        print(\"\\nDirectional confusion:\")\n",
    "        print(dir_tab)\n",
    "\n",
    "    return pred_df, g23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "082c9a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G2/G3 confusion counts:\n",
      "error_type\n",
      "correct    41\n",
      "G3→G2      11\n",
      "G2→G3       9\n",
      "other       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Cluster column: paper_Supervised.DNA.Methylation.Cluster\n",
      "================================================================================\n",
      "\n",
      "Counts (False=correct, True=G2<->G3 confused):\n",
      "wrong_g2g3                                False  True \n",
      "paper_Supervised.DNA.Methylation.Cluster              \n",
      "Classic-like                                  2      0\n",
      "Codel                                        14      7\n",
      "G-CIMP-high                                  20     12\n",
      "Mesenchymal-like                              5      0\n",
      "NA                                            1      0\n",
      "PA-like                                       2      1\n",
      "\n",
      "Per-cluster rates:\n",
      "                                          correct_rate  g2g3_confusion_rate\n",
      "paper_Supervised.DNA.Methylation.Cluster                                   \n",
      "Classic-like                                  1.000000             0.000000\n",
      "Codel                                         0.666667             0.333333\n",
      "G-CIMP-high                                   0.625000             0.375000\n",
      "Mesenchymal-like                              1.000000             0.000000\n",
      "NA                                            1.000000             0.000000\n",
      "PA-like                                       0.666667             0.333333\n",
      "\n",
      "Directional confusion:\n",
      "error_type                                G2→G3  G3→G2  correct  other\n",
      "paper_Supervised.DNA.Methylation.Cluster                              \n",
      "Classic-like                                  0      0        2      0\n",
      "Codel                                         3      4       14      0\n",
      "G-CIMP-high                                   5      7       19      1\n",
      "Mesenchymal-like                              0      0        4      1\n",
      "NA                                            0      0        0      1\n",
      "PA-like                                       1      0        2      0\n",
      "\n",
      "================================================================================\n",
      "Cluster column: paper_IDH.specific.RNA.Expression.Cluster\n",
      "================================================================================\n",
      "\n",
      "Counts (False=correct, True=G2<->G3 confused):\n",
      "wrong_g2g3                                 False  True \n",
      "paper_IDH.specific.RNA.Expression.Cluster              \n",
      "IDHmut-R1                                     12      5\n",
      "IDHmut-R2                                      5      1\n",
      "IDHmut-R3                                     17     13\n",
      "IDHwt-R1                                       1      0\n",
      "IDHwt-R2                                       2      0\n",
      "IDHwt-R3                                       3      0\n",
      "IDHwt-R4                                       3      1\n",
      "NA                                             1      0\n",
      "\n",
      "Per-cluster rates:\n",
      "                                           correct_rate  g2g3_confusion_rate\n",
      "paper_IDH.specific.RNA.Expression.Cluster                                   \n",
      "IDHmut-R1                                      0.705882             0.294118\n",
      "IDHmut-R2                                      0.833333             0.166667\n",
      "IDHmut-R3                                      0.566667             0.433333\n",
      "IDHwt-R1                                       1.000000             0.000000\n",
      "IDHwt-R2                                       1.000000             0.000000\n",
      "IDHwt-R3                                       1.000000             0.000000\n",
      "IDHwt-R4                                       0.750000             0.250000\n",
      "NA                                             1.000000             0.000000\n",
      "\n",
      "Directional confusion:\n",
      "error_type                                 G2→G3  G3→G2  correct  other\n",
      "paper_IDH.specific.RNA.Expression.Cluster                              \n",
      "IDHmut-R1                                      2      3       12      0\n",
      "IDHmut-R2                                      0      1        4      1\n",
      "IDHmut-R3                                      6      7       17      0\n",
      "IDHwt-R1                                       0      0        1      0\n",
      "IDHwt-R2                                       0      0        2      0\n",
      "IDHwt-R3                                       0      0        3      0\n",
      "IDHwt-R4                                       1      0        2      1\n",
      "NA                                             0      0        0      1\n",
      "\n",
      "================================================================================\n",
      "Cluster column: paper_Pan.Glioma.DNA.Methylation.Cluster\n",
      "================================================================================\n",
      "\n",
      "Counts (False=correct, True=G2<->G3 confused):\n",
      "wrong_g2g3                                False  True \n",
      "paper_Pan.Glioma.DNA.Methylation.Cluster              \n",
      "LGm1                                          4      1\n",
      "LGm2                                         19     14\n",
      "LGm3                                         11      4\n",
      "LGm4                                          2      0\n",
      "LGm5                                          5      0\n",
      "LGm6                                          2      1\n",
      "NA                                            1      0\n",
      "\n",
      "Per-cluster rates:\n",
      "                                          correct_rate  g2g3_confusion_rate\n",
      "paper_Pan.Glioma.DNA.Methylation.Cluster                                   \n",
      "LGm1                                          0.800000             0.200000\n",
      "LGm2                                          0.575758             0.424242\n",
      "LGm3                                          0.733333             0.266667\n",
      "LGm4                                          1.000000             0.000000\n",
      "LGm5                                          1.000000             0.000000\n",
      "LGm6                                          0.666667             0.333333\n",
      "NA                                            1.000000             0.000000\n",
      "\n",
      "Directional confusion:\n",
      "error_type                                G2→G3  G3→G2  correct  other\n",
      "paper_Pan.Glioma.DNA.Methylation.Cluster                              \n",
      "LGm1                                          1      0        3      1\n",
      "LGm2                                          6      8       19      0\n",
      "LGm3                                          1      3       11      0\n",
      "LGm4                                          0      0        2      0\n",
      "LGm5                                          0      0        4      1\n",
      "LGm6                                          1      0        2      0\n",
      "NA                                            0      0        0      1\n",
      "\n",
      "================================================================================\n",
      "Cluster column: paper_IDH.specific.DNA.Methylation.Cluster\n",
      "================================================================================\n",
      "\n",
      "Counts (False=correct, True=G2<->G3 confused):\n",
      "wrong_g2g3                                  False  True \n",
      "paper_IDH.specific.DNA.Methylation.Cluster              \n",
      "IDHmut-K1                                       4      1\n",
      "IDHmut-K2                                      16     11\n",
      "IDHmut-K3                                      14      7\n",
      "IDHwt-K1                                        2      0\n",
      "IDHwt-K2                                        5      0\n",
      "IDHwt-K3                                        2      1\n",
      "NA                                              1      0\n",
      "\n",
      "Per-cluster rates:\n",
      "                                            correct_rate  g2g3_confusion_rate\n",
      "paper_IDH.specific.DNA.Methylation.Cluster                                   \n",
      "IDHmut-K1                                       0.800000             0.200000\n",
      "IDHmut-K2                                       0.592593             0.407407\n",
      "IDHmut-K3                                       0.666667             0.333333\n",
      "IDHwt-K1                                        1.000000             0.000000\n",
      "IDHwt-K2                                        1.000000             0.000000\n",
      "IDHwt-K3                                        0.666667             0.333333\n",
      "NA                                              1.000000             0.000000\n",
      "\n",
      "Directional confusion:\n",
      "error_type                                  G2→G3  G3→G2  correct  other\n",
      "paper_IDH.specific.DNA.Methylation.Cluster                              \n",
      "IDHmut-K1                                       1      0        3      1\n",
      "IDHmut-K2                                       4      7       16      0\n",
      "IDHmut-K3                                       3      4       14      0\n",
      "IDHwt-K1                                        0      0        2      0\n",
      "IDHwt-K2                                        0      0        4      1\n",
      "IDHwt-K3                                        1      0        2      0\n",
      "NA                                              0      0        0      1\n"
     ]
    }
   ],
   "source": [
    "pred_df, g23_df = analyze_g2g3_cluster_errors(run_dir, expression_sup, samples_sup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f7536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\\fold_01\\tuner\\grade_mlp\\tuner0.json\n",
      "Reloaded trials: 254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joann\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\\tuner_hist_val_accuracy.png\n",
      "Saved: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\\tuner_corr_val_accuracy.png\n",
      "Saved: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\\tuner_box_n_layers_val_accuracy.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joann\\AppData\\Local\\Temp\\ipykernel_2760\\1270215146.py:160: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(groups, labels=labels)\n",
      "C:\\Users\\joann\\AppData\\Local\\Temp\\ipykernel_2760\\1270215146.py:160: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(groups, labels=labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\\tuner_box_dropout_val_accuracy.png\n",
      "Saved: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\\tuner_box_batch_size_val_accuracy.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joann\\AppData\\Local\\Temp\\ipykernel_2760\\1270215146.py:160: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(groups, labels=labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\\tuner_box_width_max_val_accuracy.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joann\\AppData\\Local\\Temp\\ipykernel_2760\\1270215146.py:160: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(groups, labels=labels)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "run_dir = r\"C:/Users/joann/Desktop/M2/Deep_Learning/MLP_run/20260105_025937\"\n",
    "fold_for_tuner = 1\n",
    "project_name = \"grade_mlp\"\n",
    "\n",
    "objective_name = \"val_accuracy\"\n",
    "objective_direction = \"max\"\n",
    "\n",
    "tuner_dir = os.path.join(run_dir, f\"fold_{fold_for_tuner:02d}\", \"tuner\")\n",
    "\n",
    "# Assumes already defined in session:\n",
    "# - HyperbandWithBatchSize\n",
    "# - build_model\n",
    "# - expression_sup \n",
    "input_dim = expression_sup.shape[1]\n",
    "\n",
    "tuner = HyperbandWithBatchSize(\n",
    "    hypermodel=lambda hp: build_model(hp, input_dim=input_dim, num_classes=3),\n",
    "    objective=kt.Objective(objective_name, direction=objective_direction),\n",
    "    max_epochs=100,   \n",
    "    factor=3,         \n",
    "    directory=tuner_dir,\n",
    "    project_name=project_name,\n",
    "    overwrite=False\n",
    ")\n",
    "tuner.reload()\n",
    "print(\"Reloaded trials:\", len(tuner.oracle.trials))\n",
    "\n",
    "\n",
    "# Build dataframe of trials\n",
    "\n",
    "rows = []\n",
    "for trial_id, trial in tuner.oracle.trials.items():\n",
    "    if trial.score is None:\n",
    "        continue\n",
    "    d = dict(trial.hyperparameters.values)\n",
    "    d[\"trial_id\"] = trial_id\n",
    "    d[\"score\"] = float(trial.score)  \n",
    "    d[\"status\"] = trial.status\n",
    "    rows.append(d)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "if df.empty:\n",
    "    raise ValueError(\"No trials with a score found. Check tuner_dir/project_name or objective_name.\")\n",
    "\n",
    "# stable order for plots\n",
    "df = df.sort_values(\"trial_id\").reset_index(drop=True)\n",
    "df[\"trial_num\"] = np.arange(1, len(df) + 1)\n",
    "\n",
    "# derived columns\n",
    "if \"lr\" in df.columns:\n",
    "    df[\"log_lr\"] = np.log10(df[\"lr\"].astype(float))\n",
    "if \"l2\" in df.columns:\n",
    "    df[\"log_l2\"] = np.where(df[\"l2\"].astype(float) > 0, np.log10(df[\"l2\"].astype(float)), np.nan)\n",
    "\n",
    "def reconstruct_units(row):\n",
    "    n_layers = int(row.get(\"n_layers\", 0) or 0)\n",
    "    if n_layers <= 0:\n",
    "        return []\n",
    "\n",
    "    # pyramid_half case\n",
    "    if \"base_units\" in row and pd.notna(row.get(\"base_units\")):\n",
    "        base = int(row[\"base_units\"])\n",
    "        min_u = int(row.get(\"min_units\", 8))\n",
    "        u = base\n",
    "        units = []\n",
    "        for _ in range(n_layers):\n",
    "            units.append(max(u, min_u))\n",
    "            u //= 2\n",
    "        return units\n",
    "\n",
    "    # free case\n",
    "    units = []\n",
    "    for i in range(1, n_layers + 1):\n",
    "        key = f\"units_l{i}\"\n",
    "        if key in row and pd.notna(row.get(key)):\n",
    "            units.append(int(row[key]))\n",
    "    return units\n",
    "\n",
    "df[\"width_max\"] = df.apply(lambda r: max(reconstruct_units(r), default=np.nan), axis=1)\n",
    "df[\"width_sum\"] = df.apply(lambda r: np.sum(reconstruct_units(r)) if reconstruct_units(r) else np.nan, axis=1)\n",
    "\n",
    "\n",
    "# 1) Histogram: trial val_accuracy distribution\n",
    "\n",
    "scores = df[\"score\"].to_numpy()\n",
    "best_score = scores.max()  \n",
    "mean_score = scores.mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.hist(scores, bins=20)\n",
    "ax.axvline(best_score, linestyle=\"--\", linewidth=2, label=f\"best = {best_score:.4f}\")\n",
    "ax.axvline(mean_score, linestyle=\":\", linewidth=2, label=f\"mean = {mean_score:.4f}\")\n",
    "ax.set_xlabel(objective_name)\n",
    "ax.set_ylabel(\"Count (trials)\")\n",
    "ax.set_title(f\"Distribution of {objective_name} across trials\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "out_hist = os.path.join(run_dir, f\"tuner_hist_{objective_name}.png\")\n",
    "fig.savefig(out_hist, dpi=200)\n",
    "plt.close(fig)\n",
    "print(\"Saved:\", out_hist)\n",
    "\n",
    "\n",
    "# 2) Bar plot: Spearman correlation with val_accuracy\n",
    "# Positive rho => increasing HP tends to increase val_accuracy\n",
    "\n",
    "hp_candidates = [\"log_lr\", \"dropout\", \"log_l2\", \"n_layers\", \"batch_size\", \"base_units\", \"min_units\", \"width_max\", \"width_sum\"]\n",
    "hp_candidates = [c for c in hp_candidates if c in df.columns]\n",
    "\n",
    "corrs = []\n",
    "for c in hp_candidates:\n",
    "    tmp = df[[c, \"score\"]].dropna()\n",
    "    if len(tmp) < 3:\n",
    "        continue\n",
    "    rho = tmp[c].corr(tmp[\"score\"], method=\"spearman\")\n",
    "    corrs.append((c, rho))\n",
    "\n",
    "corrs = sorted(corrs, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "labels = [c for c, _ in corrs]\n",
    "values = [rho for _, rho in corrs]\n",
    "x = np.arange(len(labels))\n",
    "ax.bar(x, values)\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(f\"Spearman ρ with {objective_name}\")\n",
    "ax.set_title(f\"Hyperparameter association with {objective_name} (signed)\")\n",
    "fig.tight_layout()\n",
    "\n",
    "out_corr = os.path.join(run_dir, f\"tuner_corr_{objective_name}.png\")\n",
    "fig.savefig(out_corr, dpi=200)\n",
    "plt.close(fig)\n",
    "print(\"Saved:\", out_corr)\n",
    "\n",
    "\n",
    "# 3) Box plots: val_accuracy grouped by key HPs\n",
    "\n",
    "def save_boxplot(group_col, out_name):\n",
    "    tmp = df[[group_col, \"score\"]].dropna().copy()\n",
    "    tmp[group_col] = tmp[group_col].astype(str)\n",
    "\n",
    "    groups = []\n",
    "    labels = []\n",
    "    for lab, g in tmp.groupby(group_col):\n",
    "        groups.append(g[\"score\"].values)\n",
    "        labels.append(lab)\n",
    "\n",
    "    # sort labels numerically if possible\n",
    "    try:\n",
    "        order = np.argsort([float(x) for x in labels])\n",
    "        labels = [labels[i] for i in order]\n",
    "        groups = [groups[i] for i in order]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.boxplot(groups, labels=labels)\n",
    "    ax.set_xlabel(group_col)\n",
    "    ax.set_ylabel(objective_name)\n",
    "    ax.set_title(f\"{objective_name} by {group_col}\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_path = os.path.join(run_dir, out_name)\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "if \"n_layers\" in df.columns:\n",
    "    save_boxplot(\"n_layers\", f\"tuner_box_n_layers_{objective_name}.png\")\n",
    "if \"dropout\" in df.columns:\n",
    "    save_boxplot(\"dropout\", f\"tuner_box_dropout_{objective_name}.png\")\n",
    "if \"batch_size\" in df.columns:\n",
    "    save_boxplot(\"batch_size\", f\"tuner_box_batch_size_{objective_name}.png\")\n",
    "\n",
    "if df[\"width_max\"].notna().any():\n",
    "    df[\"width_max_cat\"] = df[\"width_max\"].round().astype(\"Int64\").astype(str)\n",
    "    save_boxplot(\"width_max_cat\", f\"tuner_box_width_max_{objective_name}.png\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
